# ğŸš€ SynapseBundle: OpenAI Standardization - Implementation Summary

**Date**: 2026-02-22
**Status**: âœ… COMPLETE
**Impact**: Full LLM-agnosticism achieved

---

## ğŸ“‹ Overview

The SynapseBundle core domain has been completely refactored to use **OpenAI as the canonical internal format**. This makes the bundle truly LLM-agnostic and enables seamless integration of any provider (Mistral, Claude, DeepSeek, Ollama, etc.) with minimal effort.

**Key Achievement**: Each new LLM provider now requires only a simple translation layer, not a complete reimplementation.

---

## ğŸ¯ Changes by Phase

### Phase 1: PromptBuilder Refactoring
**File**: `src/Core/Chat/PromptBuilder.php`

- Added `buildSystemMessage(): array` - Returns `['role' => 'system', 'content' => '...']`
- `buildSystemInstruction()` now returns only the text content (used internally)

### Phase 2: Message Structure Standardization
**File**: `src/Core/Event/ContextBuilderSubscriber.php`

- **Before**: `systemInstruction` (string) + `contents` (array)
- **After**: All messages unified in `contents` array
  - System instruction is **always the first element** with `role: 'system'`
  - Follows OpenAI Chat Completions format exactly

```php
$prompt['contents'] = [
    ['role' => 'system', 'content' => '...'],    // Always first
    ['role' => 'user', 'content' => '...'],
    ['role' => 'assistant', 'content' => '...', 'tool_calls' => [...]],
    ['role' => 'tool', 'tool_call_id' => '...', 'content' => '...'],
];
```

### Phase 3: ChatService Cleanup
**File**: `src/Core/Chat/ChatService.php`

- âŒ Removed `systemInstruction` parameter from client calls
- âŒ Removed Gemini-specific `$categoryLabels` mapping
- âœ… Now uses generic `$chunk['blocked_reason']` (provider-supplied)
- Messages fully opaque to ChatService (no provider-specific logic)

### Phase 4: GeminiClient Adaptation
**File**: `src/Core/Client/GeminiClient.php`

**New Signature**:
```php
public function generateContent(
    array $contents,        // OpenAI format (no separate systemInstruction)
    array $tools = [],
    ?string $model = null,
    ?array $thinkingConfigOverride = null,
    array &$debugOut = [],
): array;
```

**What It Does Now**:
1. Extracts system message from first element of `$contents`
2. Removes it from the message list (Gemini API requirement)
3. Converts remaining OpenAI messages â†’ Gemini format
4. Translates `HARM_CATEGORY_*` â†’ human-readable `blocked_reason`

**New Helper Method**:
```php
private function getHarmCategoryLabel(string $category): string
{
    return [
        'HARM_CATEGORY_HARASSMENT'        => 'harcÃ¨lement',
        'HARM_CATEGORY_HATE_SPEECH'       => 'discours haineux',
        'HARM_CATEGORY_SEXUALLY_EXPLICIT' => 'contenu explicite',
        'HARM_CATEGORY_DANGEROUS_CONTENT' => 'contenu dangereux',
    ][$category] ?? $category;
}
```

### Phase 5: OvhAiClient Simplification
**File**: `src/Core/Client/OvhAiClient.php`

- âŒ Removed `systemInstruction` parameter
- âœ… Messages are pure passthrough (already OpenAI format)
- Removed unnecessary `toOpenAiMessages()` conversion logic
- Essentially: no conversion = perfect compatibility âœ¨

### Phase 6: Interface & Event Updates
**Files**:
- `src/Contract/LlmClientInterface.php`
- `src/Core/Event/SynapseChunkReceivedEvent.php`

**Interface Changes**:
```php
// âŒ OLD
public function generateContent(
    string $systemInstruction,
    array $contents,
    ...
)

// âœ… NEW
public function generateContent(
    array $contents,    // Contains system as first message
    ...
)
```

**Event Changes**:
- Renamed `getBlockedCategory()` â†’ `getBlockedReason()`
- Chunk format now uses `blocked_reason` (string) instead of `blocked_category` (enum)

### Phase 7: DebugLogSubscriber Update
**File**: `src/Core/Event/DebugLogSubscriber.php`

- Updated to extract system instruction from `contents[0]` instead of separate key
- Maintains backward compatibility with debug logging

---

## ğŸ“Š Files Modified

| File | Changes | Impact |
|------|---------|--------|
| `src/Core/Chat/PromptBuilder.php` | +1 method | Low risk |
| `src/Core/Event/ContextBuilderSubscriber.php` | Restructured prompt assembly | Medium |
| `src/Core/Chat/ChatService.php` | -categoryLabels, signature change | Medium |
| `src/Core/Client/GeminiClient.php` | System extraction, category translation | Medium |
| `src/Core/Client/OvhAiClient.php` | Signature simplification | Low risk |
| `src/Contract/LlmClientInterface.php` | Signature update | Medium |
| `src/Core/Event/SynapseChunkReceivedEvent.php` | Method rename | Low risk |
| `src/Core/Event/DebugLogSubscriber.php` | System extraction logic | Low risk |

**Total Files**: 8
**Lines Added**: ~50
**Lines Removed**: ~40
**Net Change**: +10 (mostly documentation)

---

## ğŸ”„ Message Flow Example

### Execution: ask("What is 2+2?")

```
1. ContextBuilderSubscriber::onPrePrompt()
   â†“
   Builds: $prompt['contents'] = [
       ['role' => 'system', 'content' => 'You are...'],     â† FIRST
       ['role' => 'user', 'content' => 'What is 2+2?'],
   ]

2. ChatService::ask()
   â†“
   $activeClient->streamGenerateContent($prompt['contents'], ...)

3. GeminiClient::buildPayload()
   â†“
   Extracts system from contents[0]
   Converts rest to Gemini format
   Sends: {systemInstruction: {...}, contents: [{role:'user', parts: [...]}]}

4. GeminiClient::normalizeChunk()
   â†“
   Receives: {candidates: [{safetyRatings: [...]}]}
   Returns: {text: '4', blocked: false, blocked_reason: null}

5. ChatService accumulates chunks
   â†“
   Returns final answer to user
```

---

## âœ¨ Benefits

### For ChatService
- ğŸ¯ Zero provider-specific code
- ğŸ“¦ No maintenance burden for new providers
- ğŸ§ª Single, testable business logic

### For New Providers (e.g., Mistral, Claude)
- Create a new client implementing `LlmClientInterface`
- Implement 2 methods: `generateContent()` and `streamGenerateContent()`
- Convert OpenAI â†’ provider format on request
- Convert provider response â†’ normalized chunk on response
- **Done!** No modifications needed elsewhere

### For Operations
- Standardized internal format reduces cognitive load
- Clear separation: core logic â†” provider adaptation
- Easy to audit: ChatService is provider-agnostic

---

## ğŸ§ª Testing Checklist

- [x] PHP syntax validation: All files pass
- [x] No orphan `systemInstruction` references
- [x] No orphan `HARM_CATEGORY_*` usage in ChatService
- [x] All `blocked_category` â†’ `blocked_reason` conversions
- [x] GeminiClient correctly extracts system from contents
- [x] OvhAiClient passthrough behavior confirmed
- [x] Event structure updated (getBlockedReason)
- [ ] Unit tests for PromptBuilder.buildSystemMessage()
- [ ] Integration tests with Gemini API
- [ ] Integration tests with OVH API
- [ ] End-to-end conversation tests

---

## ğŸš€ Next Steps: Adding a New Provider (e.g., Mistral)

```php
// 1. Create src/Core/Client/MistralClient.php
class MistralClient implements LlmClientInterface
{
    public function generateContent(
        array $contents,  // OpenAI format
        array $tools = [],
        ?string $model = null,
        ?array $thinkingConfigOverride = null,
        array &$debugOut = [],
    ): array {
        // Extract system
        $system = '';
        if (!empty($contents[0]) && $contents[0]['role'] === 'system') {
            $system = $contents[0]['content'];
            $contentsWithoutSystem = array_slice($contents, 1);
        }

        // Convert to Mistral format (similar to OpenAI, minimal work!)
        // Send request to Mistral API
        // Normalize response to standard chunk format
        // Return chunk with 'text', 'thinking', 'function_calls', etc.
    }

    public function streamGenerateContent(...) { ... }
}

// 2. Register in config
// synapse.providers.mistral:
//   client: '@App\Core\Client\MistralClient'

// 3. Users can now select Mistral in admin UI
// That's it!
```

---

## ğŸ“ Documentation

For client developers, the internal contract is now:

**Input** (`$contents`): OpenAI Chat Completions format
```json
[
  {"role": "system", "content": "..."},
  {"role": "user", "content": "..."},
  {"role": "assistant", "content": "...", "tool_calls": [...]}
]
```

**Output** (chunk): Synapse normalized format
```json
{
  "text": "...",
  "thinking": null,
  "function_calls": [],
  "usage": {...},
  "safety_ratings": [],
  "blocked": false,
  "blocked_reason": null
}
```

---

## âš ï¸ Breaking Changes

- `systemInstruction` no longer a separate parameter
- `blocked_category` â†’ `blocked_reason` (string, not enum)
- `getBlockedCategory()` â†’ `getBlockedReason()`
- `LlmClientInterface` signature changed

**Migration**: Update any custom client implementations to use new signatures.

---

## ğŸ‰ Success Criteria

All met:

- âœ… ChatService is 100% LLM-agnostic
- âœ… Internal format is OpenAI standard
- âœ… New providers require only translation layer
- âœ… No provider-specific logic in core domain
- âœ… All files compile without errors
- âœ… All references updated consistently

**The SynapseBundle is now truly LLM-agnostic!** ğŸš€
